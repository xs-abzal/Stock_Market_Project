{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import yfinance as yf\n",
    "import finta\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import gc; gc.enable()\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,cross_validate\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer\n",
    "from category_encoders import WOEEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as season\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define Helper Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(series):\n",
    "    '''\n",
    "    This function smoothes target/close price. \n",
    "    '''\n",
    "    series = savgol_filter(series, 21, 2)\n",
    "    series = savgol_filter(series, 11, 2)\n",
    "    series = savgol_filter(series, 11, 1)\n",
    "    return series\n",
    "    \n",
    "def loc_minima(series, order=10):\n",
    "    '''\n",
    "    Function returns pandas series with values 1&0: if local minima = 1, otherwise = 0.\n",
    "    Where,\n",
    "    series: e.g. = df.close\n",
    "    order: number of days(both sides) of the local minimum point.\n",
    "    \n",
    "    '''\n",
    "    # get the indeces of local minima with specified order(number of days from local min)\n",
    "    local_min = argrelextrema(series.values, np.less_equal, order=order)[0].tolist()\n",
    "    \n",
    "    # get pandas series with 0s.\n",
    "    min_series = 0*series\n",
    "    \n",
    "    # update series by assigning 1s to local minima(using list of indeces: local_min) \n",
    "    min_series[local_min] = 1\n",
    "    \n",
    "    gc.collect()\n",
    "    return min_series\n",
    "\n",
    "def loc_maxima(series, order=10):\n",
    "    '''\n",
    "    Function returns pandas series with values 1&0: if local maxima = 1, otherwise = 0.\n",
    "    Where,\n",
    "    series: e.g. = df.close\n",
    "    order: number of days(both sides) of the local maximum point.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # get the indeces of local maxima with specified order(number of days from local max)\n",
    "    local_max = argrelextrema(series.values, np.greater_equal, order=order)[0].tolist()\n",
    "    \n",
    "    # get pandas series with 0s.\n",
    "    max_series = 0*series\n",
    "    \n",
    "    # update series by assigning 1s to local maxima(using list of indeces: local_max) \n",
    "    max_series[local_max] = 1\n",
    "\n",
    "    gc.collect()\n",
    "    return max_series\n",
    "\n",
    "def loc_extrema(series, order=10):\n",
    "    \n",
    "    '''\n",
    "    Function returns pandas series with values 0,1,-1: \n",
    "    if local minima = 1, if local maxima = -1, otherwise = 0.\n",
    "    \n",
    "    Where,\n",
    "    series: e.g. = df.close\n",
    "    order: number of days(both sides) of the local extremum point.\n",
    "    \n",
    "    '''\n",
    "    loc_max = loc_maxima(series,order)\n",
    "    loc_min = loc_minima(series,order)\n",
    "    return loc_min - loc_max\n",
    "\n",
    "def calc_slope(x):\n",
    "    '''\n",
    "    Function calculates 'slope' of x\n",
    "    '''\n",
    "    slope = np.polyfit(range(len(x)), x, 1)[0]\n",
    "    return slope\n",
    "\n",
    "def calc_acc(x):\n",
    "    '''\n",
    "    Function calculates 'acceleration' of x\n",
    "    '''\n",
    "    acc = np.polyfit(range(len(x)), x, 2)[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def col_rename(data):\n",
    "    '''\n",
    "    Function renames columns.\n",
    "    '''\n",
    "    data.dropna(inplace=True)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.sort_values(by='Date',inplace=True)\n",
    "    data.set_index('Date',inplace=True)\n",
    "    data.index.rename('date',inplace=True)\n",
    "    data.rename(str.lower, axis='columns',inplace=True)\n",
    "    data.rename(columns={\"stock splits\": \"stock_splits\"},inplace=True)\n",
    "    #data.columns = ['close', 'dividends', 'high', 'low','open','stock_splits','ticker','volume']\n",
    "    return data\n",
    "\n",
    "def split_check(data):\n",
    "    '''\n",
    "    No splits returns True\n",
    "    if splits return False\n",
    "    '''\n",
    "    if len(data.stock_splits.unique())==1 and data.stock_splits.unique()[0]==False:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def reduce_df(data):\n",
    "    '''\n",
    "    Returns reduced data frame.\n",
    "    '''\n",
    "    data = col_rename(data) # renames columns\n",
    "    \n",
    "    try:\n",
    "        data = data[['ticker','open','high','low','close','volume']]\n",
    "        return data\n",
    "    except:\n",
    "        print('Error occured!')\n",
    "    \n",
    "    \n",
    "def display(data,x=5,y=5):\n",
    "    '''\n",
    "    Function shows head and tail of data frame.\n",
    "    '''\n",
    "    return data.head(x).append(data.tail(y))\n",
    "\n",
    "def plot(data, size = (18,6),title='Plot',legend=True):\n",
    "    '''\n",
    "    plots dataFrame or dataSeries\n",
    "    '''\n",
    "    #plt.figure(figsize=size)\n",
    "    data.plot(title=title,legend=legend,figsize=size)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_df(df, columns, size = (18,8), scale = 10*[1]):\n",
    "    \n",
    "    '''\n",
    "    Plots dataFrame or dataSeries.\n",
    "    '''\n",
    "\n",
    "    #plt.style.use('fivethirtyeight')\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('tab10')\n",
    "\n",
    "    plt.figure(figsize=size)\n",
    "\n",
    "    #multiple line plot\n",
    "    #plot main line\n",
    "    plt.plot(df.index, scale[0]*df[columns[0]], marker='', color='blue', linewidth=3,\n",
    "             alpha=1, label = columns[0], linestyle='solid')\n",
    "    \n",
    "    #plot secondary lines\n",
    "    num=0\n",
    "    for col in df[columns].drop(columns[0], axis=1):\n",
    "        num+=1\n",
    "        plt.plot(df.index, scale[num]*df[col], marker='', color=palette(num), linewidth=1, \n",
    "                 alpha=0.7, label=col, linestyle='solid') #markersize=1\n",
    "\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Change xlim\n",
    "    #plt.xlim(0,12)\n",
    "\n",
    "    # Add titles\n",
    "    plt.title(\"Plot\", fontsize=20, fontweight=3, color='black')\n",
    "    plt.xlabel(\"Date\",fontsize=14, fontweight=3, color='black')\n",
    "    plt.ylabel(\"Close Price\",fontsize=14, fontweight=3, color='black')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def add_features(df):\n",
    "    \n",
    "    '''\n",
    "    Function returns data frame with features.\n",
    "    '''\n",
    "       \n",
    "    ##### Prepare columns for finta module https://pypi.org/project/finta/\n",
    "    ohlc = ['open', 'high', 'low', 'close']\n",
    "    ohlcv = ['open', 'high', 'low', 'close', 'volume']\n",
    "    \n",
    "    df_accum = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    ##### Loop through\n",
    "    for ticker in df['ticker'].unique():\n",
    "        \n",
    "        df_tmp = df.loc[df[\"ticker\"]==ticker]\n",
    "\n",
    "        ##### Get close price change for 1-15, 20, 25, 30 days ago\n",
    "\n",
    "        # Percentage change between the current and a prior closing price\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pct_change.html\n",
    "\n",
    "        # Compare percentage change of today's price refernce to price of'lag' day ago\n",
    "        lags = list(range(1,16))+[20,25,30]\n",
    "        for i in lags:\n",
    "            df_tmp[f'pct_lag_{i}']=df_tmp['close'].pct_change(periods=i)\n",
    "\n",
    "\n",
    "\n",
    "        ##### Get the rolling mean and std deviation of pct_change\n",
    "\n",
    "        # Compare percentage change of today's price refernce to price'lag' day ago\n",
    "        lags = [1,5,10,15,10] \n",
    "\n",
    "        # Rolling windows = 5,10,15,20 days\n",
    "        windows = [5,10,15,20] \n",
    "        for i in lags:\n",
    "            for window in windows:\n",
    "                df_tmp[f'pct_r_mean_{i}_{window}'] = df_tmp[f'pct_lag_{i}'].rolling(window).mean()\n",
    "                df_tmp[f'pct_r_std_{i}_{window}'] = df_tmp[f'pct_lag_{i}'].rolling(window).std()\n",
    "\n",
    "\n",
    "        ##### Get the exponential mean and std deviation of pct_change\n",
    "        # finta.TA.EMA(df[ohlc],period=5) is the same df.close.ewm(span = 5).mean()\n",
    "\n",
    "        # use pct_lag_1 here\n",
    "        # use span = 5 and 10 periods or alpha = 2/(span+1)  (period is not the same as day)        \n",
    "        df_tmp['pct_exp_mean_1_5'] = df_tmp['pct_lag_1'].ewm(span=5).mean()\n",
    "        df_tmp['pct_exp_std_1_5'] = df_tmp['pct_lag_1'].ewm(span=5).std()\n",
    "\n",
    "        df_tmp['pct_exp_mean_1_10'] = df_tmp['pct_lag_1'].ewm(span=10).mean()\n",
    "        df_tmp['pct_exp_std_1_10'] = df_tmp['pct_lag_1'].ewm(span=10).std()\n",
    "\n",
    "\n",
    "        ##### Get 'Direction'='MACD' - 'Signal'\n",
    "\n",
    "        # MACD= 12_Period EMA − 26_Period EMA\n",
    "        # https://www.investopedia.com/terms/m/macd.asp\n",
    "        # df.close.ewm(span=12).mean()-df.close.ewm(span=26).mean() is the same as:\n",
    "        # finta.TA.MACD(df[ohlc]), also returns 9 day signal line\n",
    "\n",
    "        df_tmp['macd'] = (df_tmp.close.ewm(span=12).mean()-df_tmp.close.ewm(span=26).mean())\n",
    "\n",
    "        # signal line is 9 period EMA of the MACD line\n",
    "        df_tmp['signal']= df_tmp['macd'].ewm(span=9).mean()\n",
    "\n",
    "        # diffrence between MACD and Signal line indicates bullish\n",
    "        # or bearish direction of the market\n",
    "        df_tmp['direction'] = df_tmp['macd']-df_tmp['signal']\n",
    "\n",
    "\n",
    "        ##### Get 'slope' and 'acceleration'                          \n",
    "        df_tmp['slope'] = df_tmp.close.rolling(9).apply(calc_slope,raw=False)\n",
    "        df_tmp['acc'] = df_tmp.close.rolling(11).apply(calc_acc,raw=False)\n",
    "\n",
    "\n",
    "        ### Get parabolic sar: 'stop and reverse'\n",
    "        # https://www.investopedia.com/trading/introduction-to-parabolic-sar/\n",
    "        # e.g value of af = 0.01  could keep in position longer time \n",
    "        df_tmp['sar'] = finta.TA.SAR(df_tmp[ohlc].copy(), af=0.025, amax=0.15)# not use as feature, plot only  \n",
    "\n",
    "        # get normalized sar as signal: \n",
    "        #if 0<norm<1 then uptrend, if norm < 0, downtrend\n",
    "        # here is an imballance possible, however as we do two separate models, \n",
    "        # for min and max target, may need to adjust\n",
    "        df_tmp['sar_norm'] = 1- df_tmp.sar/df_tmp.close\n",
    "\n",
    "        # do the same for moving avg 50,100,150,200\n",
    "        windows=[50,100,150,200]\n",
    "\n",
    "        # get normalized rolling_mean as signal: if 0<norm<1 then uptrend, if norm < 0, downtrend\n",
    "        # here is an imballance possible, however as we do two separate models, \n",
    "        # for min and max target\n",
    "        for win in windows: \n",
    "            df_tmp[f'r_mean_{win}'] = df_tmp.close.rolling(win).mean() # not use as feature, plot only\n",
    "            df_tmp[f'r_mean_{win}_norm'] = 1 - df_tmp[f'r_mean_{win}']/df_tmp.close\n",
    "\n",
    "        # macd: if short moving avg > long moving avg \n",
    "        # normalize: divide by short moving avg, and if 0<norm<1 then uptrend, if norm < 0, downtrend\n",
    "\n",
    "        df_tmp[f'macd_50_100_norm'] = 1 - df_tmp[f'r_mean_100']/df_tmp[f'r_mean_50']\n",
    "        df_tmp[f'macd_50_150_norm'] = 1 - df_tmp[f'r_mean_150']/df_tmp[f'r_mean_50']\n",
    "        df_tmp[f'macd_50_200_norm'] = 1 - df_tmp[f'r_mean_200']/df_tmp[f'r_mean_50']\n",
    "        df_tmp[f'macd_100_200_norm'] = 1 - df_tmp[f'r_mean_200']/df_tmp[f'r_mean_100']\n",
    "\n",
    "\n",
    "        ### Get ADX: Average Directional Index\n",
    "        # The A.D.X. is 100 * smoothed moving average of absolute value (DMI +/-) \n",
    "        # divided by (DMI+ + DMI-). ADX does not indicate trend direction or momentum,\n",
    "        # only trend strength.\n",
    "        # Here I normalised by dividing 30(assumed here above 30 is moderately strong trend).\n",
    "        # https://www.investopedia.com/terms/a/adx.asp\n",
    "        df_tmp['adx'] = (finta.TA.ADX(df_tmp[ohlc].copy(), period=14))/30\n",
    "\n",
    "\n",
    "        ### Get RSI relative strength index is a momentum indicator\n",
    "        # https://www.investopedia.com/terms/r/rsi.asp\n",
    "        df_tmp['rsi'] = finta.TA.RSI(df_tmp[ohlc].copy(), period=14)\n",
    "        \n",
    "#         df_tmp['vwap'] = finta.TA.VWAP(df_tmp[ohlcv].copy())\n",
    "        \n",
    "        ### add_targets\n",
    "        # get local minima and maxima (10 days both side from local extrema)\n",
    "        df_tmp['target_min'] = loc_minima(df_tmp.close,order=10)\n",
    "        df_tmp['target_max'] = loc_maxima(df_tmp.close,order=10)\n",
    "\n",
    "        # get local extrema, its the same as above, just for plotting purpose\n",
    "        # local minima(=1) and maxima(=-1)\n",
    "        df_tmp['extrema'] = loc_extrema(df_tmp.close,order=10)\n",
    "\n",
    "        #df_tmp.dropna(inplace=True)\n",
    "        \n",
    "        df_accum = pd.concat([df_accum,df_tmp],sort='False')\n",
    "\n",
    "    df_accum.dropna(inplace=True)\n",
    "    df_accum.sort_values(by='date',inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_accum\n",
    "\n",
    "\n",
    "def roi_calc(data):\n",
    "    \n",
    "    '''\n",
    "    Function calculates ROI for each stock ticker.\n",
    "    Returns two dataframes: \n",
    "    - ROI for each stock for all test period.\n",
    "    - DF with positions in and out.\n",
    "    '''\n",
    "    \n",
    "    data = data[['ticker','open','high','low','close','predicted']]\n",
    "    data['pos_in']= data['predicted']\n",
    "    data['pos_out'] = 0\n",
    "    data['roi']=1\n",
    "    data.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "    t_stop = -0.045 #### trail stop loss 4.5%\n",
    "    h_stop = 0.95 # hard stop loss 5% per initial buy\n",
    "\n",
    "    # create empty data frame, will use for assemble back all stock tickers\n",
    "    concat_all_df = pd.DataFrame()\n",
    "\n",
    "    #loop thru each ticker as df-s\n",
    "    for tick in data['ticker'].unique():\n",
    "\n",
    "        # create data frame for each stock ticker\n",
    "        ticker_df = data.loc[data['ticker']==tick]\n",
    "\n",
    "        # reset indeces\n",
    "        ticker_df.reset_index(inplace=True)\n",
    "\n",
    "        # index of the first position_in=1\n",
    "        first_idx = ticker_df.loc[ticker_df['pos_in']==1].index[0]\n",
    "\n",
    "        # start df with first in_position = 1\n",
    "        ticker_df = ticker_df[first_idx:]\n",
    "\n",
    "        # fix indeces\n",
    "        ticker_df.reset_index(inplace=True)\n",
    "        del ticker_df['index']\n",
    "\n",
    "        # get the list of  all position_in global indeces for one ticker,\n",
    "        # e.g., for 'BMY'.\n",
    "        idx_pos_in = ticker_df.loc[ticker_df.pos_in ==1].index.tolist()\n",
    "\n",
    "        # add the last index to the list if it is not in_postion\n",
    "        if (len(ticker_df)-1) not in idx_pos_in:\n",
    "            idx_pos_in.append(len(ticker_df)-1)\n",
    "\n",
    "        # create empty data frame, will use for assemble back\n",
    "        concat_df = pd.DataFrame()\n",
    "\n",
    "        # loop thru all indeces of the list( which is indeces of postion_in) \n",
    "        # for one ticker/symbol, e.g. 'BMY'\n",
    "        for i in range(len(idx_pos_in)):\n",
    "\n",
    "            # to avoid out of range for the last section of position_in \n",
    "            # till the end of the data frame \n",
    "            if i < len(idx_pos_in)-1:\n",
    "\n",
    "                # start slicing the data frame of one ticker for multiple dataframes, \n",
    "                # based on the position_in, end - start is delta \n",
    "                # between one postion_in and next position_in\n",
    "                start = idx_pos_in[i] # global index of position_in\n",
    "                end = idx_pos_in[i+1] # global index of next position_in\n",
    "\n",
    "                # create a copy of a sub data frame(from one position_in till next position_in )\n",
    "                tmp = ticker_df.loc[start:end].copy() \n",
    "\n",
    "                # create trailing loss column\n",
    "                tmp['pct_loss'] = tmp['close'].pct_change()\n",
    "\n",
    "                # fill NaN (created after pct_change) with 0\n",
    "                tmp.fillna(0,inplace=True)\n",
    "\n",
    "                # loop inside this sub data frame by indeces (j = index)\n",
    "                for j in range(start, end+1):\n",
    "\n",
    "                    # hard loss today(today means at global index j)\n",
    "                    h_loss = tmp['close'][j]/tmp['close'][start]\n",
    "                    # trail loss today\n",
    "                    t_loss = tmp['pct_loss'][j] \n",
    "\n",
    "                    # define if need to stop; \n",
    "                    # e.g., h_loss = 0.95 or t_loss = -0.07 reqire to stop\n",
    "                    # as it excced limit specified above as h_stop and t_stopp\n",
    "                    if h_loss <= h_stop or t_loss <= t_stop:\n",
    "\n",
    "                        # assign to postion_out 1 as we exceeded allowed stop loss\n",
    "                        tmp.loc[j,'pos_out']=1\n",
    "\n",
    "                        # assign to ROI current h_loss, as it is equal to ROI\n",
    "                        tmp.loc[j,'roi'] = h_loss\n",
    "\n",
    "                        # break for 'j' loop\n",
    "                        break\n",
    "\n",
    "                    # if we didnt exceed any loss stops, than last \n",
    "                    # position_out = position_in=1\n",
    "                    # than means we exit postion and enter it again with the same closed price\n",
    "                    # its 'fake' position close\n",
    "                    if j==(end):\n",
    "                        tmp.loc[j,'pos_out']=1\n",
    "                        tmp.loc[j,'roi'] = h_loss\n",
    "\n",
    "                # remove first row of the sub data frame, starting from the second\n",
    "                if i>0:\n",
    "                    tmp = tmp[1:]\n",
    "                concat_df = pd.concat([concat_df,tmp],join='outer')\n",
    "\n",
    "        concat_df['roi_cumprod']=concat_df['roi'].cumprod()\n",
    "        \n",
    "        # dataframe with all data, including roi for each stock\n",
    "        concat_all_df = pd.concat([concat_all_df,concat_df],join='outer')\n",
    "    \n",
    "    del concat_df, tmp\n",
    "    gc.collect()\n",
    "    \n",
    "    roi_all_df = concat_all_df.groupby('ticker')['roi_cumprod'].agg(['last'])\n",
    "    roi_all_df.columns=['roi']\n",
    "    \n",
    "    return roi_all_df, concat_all_df\n",
    "\n",
    "def calc_sar(data):\n",
    "    '''\n",
    "    This function returns ROI using SAR for each stock-ticker.\n",
    "    '''\n",
    "    ohlc= ['open', 'high', 'low', 'close']\n",
    "    data = data[ohlc+['ticker']]\n",
    "\n",
    "\n",
    "    # create empty data frame, will use for assemble back all stock tickers\n",
    "    all_sar_data= pd.DataFrame()\n",
    "\n",
    "    #loop thru each ticker as df-s\n",
    "    for tick in data['ticker'].unique():\n",
    "        # create data frame for each stock ticker\n",
    "        ticker_df = data.loc[data['ticker']==tick]\n",
    "\n",
    "        # typically af=0.01 for stocks and af=0.02 for currencies (forex)\n",
    "        # but pennies and small-caps behaviour is closer to currencies than stocks\n",
    "        # https://www.investopedia.com/trading/introduction-to-parabolic-sar/\n",
    "\n",
    "        ticker_df['sar'] = finta.TA.SAR(ticker_df[ohlc], af=0.025, amax=0.15) \n",
    "        # e.g value of af = 0.01  could keep in position longer time \n",
    "\n",
    "        ticker_df['position'] = (ticker_df.sar < ticker_df.close).astype(np.int)\n",
    "\n",
    "        #https://www.investopedia.com/terms/a/adx.asp\n",
    "        ticker_df['adx'] = (finta.TA.ADX(ticker_df[ohlc], 14))/30\n",
    "\n",
    "        ticker_df['region'] = (ticker_df.position != ticker_df.position.shift()).cumsum()\n",
    "\n",
    "        stats = ticker_df.loc[ticker_df.position == 1].groupby(['ticker','region'])[['close']].agg(['first', 'last', 'count'])\n",
    "        stats.columns = ['opened', 'closed', 'days']\n",
    "\n",
    "        # Return on investment\n",
    "        stats['roi'] = stats.closed / stats.opened \n",
    "\n",
    "        # Profits & Losses\n",
    "        stats['pnl'] = stats.roi - 1.0\n",
    "\n",
    "        # Profits at a fixed amount - trading without reinvestment\n",
    "        stats['pnl_cumulat'] = stats['pnl'].cumsum()\n",
    "\n",
    "        # Profits at 100% portfolio value - trading with 100% reinvestment\n",
    "        stats['roi_compound'] = stats['roi'].cumprod()\n",
    "        stats = stats[-1:]\n",
    "\n",
    "\n",
    "        # dataframe with all data, including roi for each stock\n",
    "        all_sar_data = pd.concat([all_sar_data,stats],join='outer')\n",
    "\n",
    "        del ticker_df, stats\n",
    "        gc.collect()\n",
    "    all_sar_data.reset_index(inplace=True)\n",
    "    \n",
    "    return all_sar_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get and prepare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1. Get from stock_500_full.csv.\n",
    "# df = pd.read_csv('stocks_500_full.csv')\n",
    "# df = reduce_df(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # leave only stock tickers which have data from 2010 and till 2020-05-18 \n",
    "# start_list = df['2010'].ticker.unique().tolist()\n",
    "# df = df.loc[df.ticker.isin(start_list)]\n",
    "\n",
    "# end_list = df['2020-05-18':].ticker.unique().tolist()\n",
    "# df = df.loc[df.ticker.isin(end_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.ticker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Get 3 stocks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shares_3=['BMY','OHI','EVRG']\n",
    "# df_3 = df.loc[df['ticker'].isin(shares_3)]\n",
    "# df_3.ticker.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Get S&P 500 index for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2010-05-24</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1084.78</td>\n",
       "      <td>1089.95</td>\n",
       "      <td>1072.70</td>\n",
       "      <td>1073.65</td>\n",
       "      <td>5224040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-05-25</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1067.42</td>\n",
       "      <td>1074.75</td>\n",
       "      <td>1040.78</td>\n",
       "      <td>1074.03</td>\n",
       "      <td>7329580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-05-26</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1075.51</td>\n",
       "      <td>1090.75</td>\n",
       "      <td>1065.59</td>\n",
       "      <td>1067.95</td>\n",
       "      <td>4521050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1074.27</td>\n",
       "      <td>1103.52</td>\n",
       "      <td>1074.27</td>\n",
       "      <td>1103.06</td>\n",
       "      <td>5698460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010-05-28</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1102.59</td>\n",
       "      <td>1102.59</td>\n",
       "      <td>1084.78</td>\n",
       "      <td>1089.41</td>\n",
       "      <td>4871210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2913.86</td>\n",
       "      <td>2968.09</td>\n",
       "      <td>2913.86</td>\n",
       "      <td>2953.91</td>\n",
       "      <td>6364290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-19</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2948.59</td>\n",
       "      <td>2964.21</td>\n",
       "      <td>2922.35</td>\n",
       "      <td>2922.94</td>\n",
       "      <td>4969330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2953.63</td>\n",
       "      <td>2980.29</td>\n",
       "      <td>2953.63</td>\n",
       "      <td>2971.61</td>\n",
       "      <td>4992970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-21</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2969.95</td>\n",
       "      <td>2978.50</td>\n",
       "      <td>2938.57</td>\n",
       "      <td>2948.51</td>\n",
       "      <td>4966940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2948.05</td>\n",
       "      <td>2956.76</td>\n",
       "      <td>2933.59</td>\n",
       "      <td>2955.45</td>\n",
       "      <td>3952800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2518 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ticker     open     high      low    close      volume\n",
       "date                                                             \n",
       "2010-05-24  ^GSPC  1084.78  1089.95  1072.70  1073.65  5224040000\n",
       "2010-05-25  ^GSPC  1067.42  1074.75  1040.78  1074.03  7329580000\n",
       "2010-05-26  ^GSPC  1075.51  1090.75  1065.59  1067.95  4521050000\n",
       "2010-05-27  ^GSPC  1074.27  1103.52  1074.27  1103.06  5698460000\n",
       "2010-05-28  ^GSPC  1102.59  1102.59  1084.78  1089.41  4871210000\n",
       "...           ...      ...      ...      ...      ...         ...\n",
       "2020-05-18  ^GSPC  2913.86  2968.09  2913.86  2953.91  6364290000\n",
       "2020-05-19  ^GSPC  2948.59  2964.21  2922.35  2922.94  4969330000\n",
       "2020-05-20  ^GSPC  2953.63  2980.29  2953.63  2971.61  4992970000\n",
       "2020-05-21  ^GSPC  2969.95  2978.50  2938.57  2948.51  4966940000\n",
       "2020-05-22  ^GSPC  2948.05  2956.76  2933.59  2955.45  3952800000\n",
       "\n",
       "[2518 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snp = pd.read_csv('../data/snp500.csv')\n",
    "snp = reduce_df(snp)\n",
    "snp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add features to data frame-snp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_3 = add_features(df_3)\n",
    "snp = add_features(snp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>adx</th>\n",
       "      <th>close</th>\n",
       "      <th>direction</th>\n",
       "      <th>extrema</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_100_200_norm</th>\n",
       "      <th>macd_50_100_norm</th>\n",
       "      <th>...</th>\n",
       "      <th>r_mean_50_norm</th>\n",
       "      <th>rsi</th>\n",
       "      <th>sar</th>\n",
       "      <th>sar_norm</th>\n",
       "      <th>signal</th>\n",
       "      <th>slope</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_min</th>\n",
       "      <th>ticker</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2011-03-08</td>\n",
       "      <td>-0.061200</td>\n",
       "      <td>0.760854</td>\n",
       "      <td>1321.82</td>\n",
       "      <td>-2.472337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1325.74</td>\n",
       "      <td>1306.86</td>\n",
       "      <td>5.244644</td>\n",
       "      <td>0.061702</td>\n",
       "      <td>0.035277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017358</td>\n",
       "      <td>52.505082</td>\n",
       "      <td>1297.966950</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>7.716982</td>\n",
       "      <td>0.768833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>4531420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2011-03-09</td>\n",
       "      <td>-0.188240</td>\n",
       "      <td>0.729554</td>\n",
       "      <td>1320.02</td>\n",
       "      <td>-2.196815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1323.21</td>\n",
       "      <td>1312.27</td>\n",
       "      <td>4.970963</td>\n",
       "      <td>0.061814</td>\n",
       "      <td>0.035080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>51.180036</td>\n",
       "      <td>1299.682603</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>7.167778</td>\n",
       "      <td>0.077833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>3709520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2011-03-10</td>\n",
       "      <td>-0.543427</td>\n",
       "      <td>0.752665</td>\n",
       "      <td>1295.11</td>\n",
       "      <td>-3.564007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1315.72</td>\n",
       "      <td>1294.21</td>\n",
       "      <td>2.712769</td>\n",
       "      <td>0.061822</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004438</td>\n",
       "      <td>36.479627</td>\n",
       "      <td>1332.280000</td>\n",
       "      <td>-0.028700</td>\n",
       "      <td>6.276776</td>\n",
       "      <td>-1.357500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>4723020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>2.068298</td>\n",
       "      <td>0.765164</td>\n",
       "      <td>2971.61</td>\n",
       "      <td>3.509324</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2980.29</td>\n",
       "      <td>2953.63</td>\n",
       "      <td>40.066818</td>\n",
       "      <td>-0.007927</td>\n",
       "      <td>-0.094783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085401</td>\n",
       "      <td>64.712850</td>\n",
       "      <td>2776.712500</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>36.557494</td>\n",
       "      <td>5.946167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>4992970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-21</td>\n",
       "      <td>2.771970</td>\n",
       "      <td>0.738762</td>\n",
       "      <td>2948.51</td>\n",
       "      <td>3.722935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2978.50</td>\n",
       "      <td>2938.57</td>\n",
       "      <td>41.211163</td>\n",
       "      <td>-0.009028</td>\n",
       "      <td>-0.092046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076831</td>\n",
       "      <td>59.715110</td>\n",
       "      <td>2791.980812</td>\n",
       "      <td>0.053088</td>\n",
       "      <td>37.488228</td>\n",
       "      <td>11.417333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>4966940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>2.746958</td>\n",
       "      <td>0.706997</td>\n",
       "      <td>2955.45</td>\n",
       "      <td>3.762781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2956.76</td>\n",
       "      <td>2933.59</td>\n",
       "      <td>42.191704</td>\n",
       "      <td>-0.010051</td>\n",
       "      <td>-0.087277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075785</td>\n",
       "      <td>60.765490</td>\n",
       "      <td>2806.104002</td>\n",
       "      <td>0.050532</td>\n",
       "      <td>38.428923</td>\n",
       "      <td>17.071833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>3952800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 acc       adx    close  direction  extrema     high      low  \\\n",
       "date                                                                            \n",
       "2011-03-08 -0.061200  0.760854  1321.82  -2.472337      0.0  1325.74  1306.86   \n",
       "2011-03-09 -0.188240  0.729554  1320.02  -2.196815      0.0  1323.21  1312.27   \n",
       "2011-03-10 -0.543427  0.752665  1295.11  -3.564007      0.0  1315.72  1294.21   \n",
       "2020-05-20  2.068298  0.765164  2971.61   3.509324     -1.0  2980.29  2953.63   \n",
       "2020-05-21  2.771970  0.738762  2948.51   3.722935      0.0  2978.50  2938.57   \n",
       "2020-05-22  2.746958  0.706997  2955.45   3.762781      0.0  2956.76  2933.59   \n",
       "\n",
       "                 macd  macd_100_200_norm  macd_50_100_norm  ...  \\\n",
       "date                                                        ...   \n",
       "2011-03-08   5.244644           0.061702          0.035277  ...   \n",
       "2011-03-09   4.970963           0.061814          0.035080  ...   \n",
       "2011-03-10   2.712769           0.061822          0.034709  ...   \n",
       "2020-05-20  40.066818          -0.007927         -0.094783  ...   \n",
       "2020-05-21  41.211163          -0.009028         -0.092046  ...   \n",
       "2020-05-22  42.191704          -0.010051         -0.087277  ...   \n",
       "\n",
       "            r_mean_50_norm        rsi          sar  sar_norm     signal  \\\n",
       "date                                                                      \n",
       "2011-03-08        0.017358  52.505082  1297.966950  0.018046   7.716982   \n",
       "2011-03-09        0.015071  51.180036  1299.682603  0.015407   7.167778   \n",
       "2011-03-10       -0.004438  36.479627  1332.280000 -0.028700   6.276776   \n",
       "2020-05-20        0.085401  64.712850  2776.712500  0.065587  36.557494   \n",
       "2020-05-21        0.076831  59.715110  2791.980812  0.053088  37.488228   \n",
       "2020-05-22        0.075785  60.765490  2806.104002  0.050532  38.428923   \n",
       "\n",
       "                slope  target_max  target_min  ticker      volume  \n",
       "date                                                               \n",
       "2011-03-08   0.768833         0.0         0.0   ^GSPC  4531420000  \n",
       "2011-03-09   0.077833         0.0         0.0   ^GSPC  3709520000  \n",
       "2011-03-10  -1.357500         0.0         0.0   ^GSPC  4723020000  \n",
       "2020-05-20   5.946167         1.0         0.0   ^GSPC  4992970000  \n",
       "2020-05-21  11.417333         0.0         0.0   ^GSPC  4966940000  \n",
       "2020-05-22  17.071833         0.0         0.0   ^GSPC  3952800000  \n",
       "\n",
       "[6 rows x 84 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(snp,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_3,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modelling.\n",
    "### Prepare Features(X) and target local minima(y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['pct_lag_1','pct_lag_2', 'pct_lag_3', 'pct_lag_4', \n",
    "             'pct_lag_5', 'pct_lag_6','pct_lag_7', 'pct_lag_8', 'pct_lag_9', \n",
    "             'pct_lag_10', 'pct_lag_11','pct_lag_12', 'pct_lag_13', 'pct_lag_14',\n",
    "             'pct_lag_15', 'pct_lag_20', 'pct_lag_25', 'pct_lag_30',\n",
    "             'pct_r_mean_1_5','pct_r_std_1_5','pct_r_mean_1_10', 'pct_r_std_1_10',\n",
    "             'pct_r_mean_1_15', 'pct_r_std_1_15', 'pct_r_mean_1_20', 'pct_r_std_1_20',\n",
    "             'pct_r_mean_5_5','pct_r_std_5_5', 'pct_r_mean_5_10', 'pct_r_std_5_10',\n",
    "             'pct_r_mean_5_15','pct_r_std_5_15', 'pct_r_mean_5_20', 'pct_r_std_5_20',\n",
    "             'pct_r_mean_10_5', 'pct_r_std_10_5', 'pct_r_mean_10_10','pct_r_std_10_10',\n",
    "             'pct_r_mean_10_15', 'pct_r_std_10_15','pct_r_mean_10_20', 'pct_r_std_10_20', \n",
    "             'pct_r_mean_15_5','pct_r_std_15_5', 'pct_r_mean_15_10', 'pct_r_std_15_10',\n",
    "             'pct_r_mean_15_15', 'pct_r_std_15_15', 'pct_r_mean_15_20','pct_r_std_15_20',\n",
    "             'pct_exp_mean_1_5', 'pct_exp_std_1_5','pct_exp_mean_1_10', 'pct_exp_std_1_10', \n",
    "             'r_mean_50_norm','r_mean_100_norm','r_mean_150_norm','r_mean_200_norm',\n",
    "             'macd_50_100_norm', 'macd_50_150_norm','macd_50_200_norm', 'macd_100_200_norm',\n",
    "             'direction','slope', 'acc', 'sar_norm', 'adx', 'rsi'] \n",
    "\n",
    "cat_cols = ['ticker']\n",
    "\n",
    "used_cols = num_cols + cat_cols\n",
    "\n",
    "### for df_3\n",
    "# X = df_3[used_cols]\n",
    "# y = df_3['target_min']\n",
    "\n",
    "### for snp\n",
    "X_snp = snp[used_cols]\n",
    "y_snp = snp['target_min']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Test on S&P 500 data.\n",
    "### Vanilla LogReg model for snp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Make numerical pipe\n",
    "# imputer = SimpleImputer()\n",
    "# scaler = RobustScaler()\n",
    "# pca = PCA(n_components=0.95, random_state = 42)\n",
    "# num_pipe = make_pipeline(imputer,scaler,pca)\n",
    "\n",
    "# ### Make categorical pipe\n",
    "# encoder = WOEEncoder()\n",
    "# cat_pipe = make_pipeline(encoder)\n",
    "\n",
    "# # Define preprocessor\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[('num', num_pipe, num_cols),\n",
    "#                   ('cat', cat_pipe, cat_cols)])\n",
    "\n",
    "# # Define model\n",
    "# model = LogisticRegression(class_weight='balanced', random_state=42,C=10,solver = 'newton-cg',\n",
    "#                           fit_intercept=True,penalty='l2')\n",
    "\n",
    "# # Make main pipeline\n",
    "# pipe_snp = make_pipeline(preprocessor, model)\n",
    "\n",
    "# # Get cross val score\n",
    "# scores = cross_val_score(pipe, X_snp, y_snp, cv=5, scoring='roc_auc')\n",
    "\n",
    "# # Print results\n",
    "# print(f'ROC-AUC scores:     {scores}')\n",
    "# print('-------------------')\n",
    "# print(f'Mean ROC-AUC score: {scores.mean()} +/-{scores.std()}')\n",
    "# print('-------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg Score Card model for snp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make numerical pipe\n",
    "# imputer = SimpleImputer()\n",
    "# binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "# scaler = RobustScaler()\n",
    "# encoder = WOEEncoder()\n",
    "# pca = PCA(n_components=0.90, random_state = 42)\n",
    "# num_pipe = make_pipeline(imputer, binner, scaler, encoder)#,pca)\n",
    "\n",
    "# # Make categorical pipe\n",
    "# encoder = WOEEncoder()\n",
    "# cat_pipe = make_pipeline(encoder)\n",
    "\n",
    "# # Define preprocessor\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[('num', num_pipe, num_cols),\n",
    "#                   ('cat', cat_pipe, cat_cols)])\n",
    "\n",
    "# # Define model\n",
    "# model = LogisticRegression(class_weight='balanced', random_state=42,C=10,solver = 'lbfgs',\n",
    "#                           fit_intercept=True,penalty='l2')\n",
    "\n",
    "# # Make main pipeline\n",
    "# pipe= make_pipeline(preprocessor, model)\n",
    "\n",
    "# # Get cross val score\n",
    "# scores = cross_val_score(pipe, X_snp, y_snp, cv=5, scoring='roc_auc')\n",
    "\n",
    "# # Print results\n",
    "# print(f'ROC-AUC scores:     {scores}')\n",
    "# print('-------------------')\n",
    "# print(f'Mean ROC-AUC score: {scores.mean()} +/-{scores.std()}')\n",
    "# print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split for snp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.model_selection import train_test_split\n",
    "# end = '2017-05-18'\n",
    "# start = '2017-05-19'\n",
    "\n",
    "# # for snp\n",
    "# X_train, X_test = X_snp[:end],X_snp[start:] \n",
    "# y_train, y_test= y_snp[:end],y_snp[start:]  \n",
    "\n",
    "# snp_trained = snp[:end]\n",
    "# snp_tested = snp[start:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train scorecard for snp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.fit(X_train, y_train)\n",
    "\n",
    "# y_hat_train = pipe.predict(X_train)\n",
    "# y_hat_test = pipe.predict(X_test)\n",
    "\n",
    "# prob_train = pipe.predict_proba(X_train)[:,1]\n",
    "# prob_test = pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "# print(f'ROC-AUC train_scores: {roc_auc_score(y_train, prob_train)}')\n",
    "# print('---------------------')\n",
    "# print(f'ROC-AUC test_scores: {roc_auc_score(y_test, prob_test)}')\n",
    "# print('---------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = 0.8\n",
    "# pred_train = (prob_train >= thresh).astype(np.int)\n",
    "# print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(y_train,pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = 0.8\n",
    "# pred = (prob_test >= thresh).astype(np.int)\n",
    "# print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add predictions to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end = '2017-05-18'\n",
    "# start = '2017-05-19'\n",
    "\n",
    "# snp_trained = snp[:end]\n",
    "# snp_trained['predicted']=pred_train\n",
    "\n",
    "# snp_tested = snp[start:]\n",
    "# snp_tested['predicted']=pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snp_roi, snp_concat = roi_calc(snp_tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snp_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snp_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare with long term investment\n",
    "# snp_long = (snp_concat['close'][-1:].values/snp_concat['close'][:1].values)[0]\n",
    "\n",
    "# print(snp_long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
